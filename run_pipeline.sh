#!/bin/bash

# Configuration
HDFS_BASE_PATH="hdfs://127.0.0.1:9000/hepmass"
SCRIPT_DIR="scripts"
LOG_CONFIG="log4j.properties"
VERSION="1.2.0"
HADOOP_SBIN="${HADOOP_HOME}/sbin"
SPARK_SBIN="${SPARK_HOME}/sbin"

# Codes couleur
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Variables d'Ã©tat
ENABLE_LOGS=false
MANAGE_SERVICES=true
DISABLE_SAFEMODE=false
SPARK_LOG_OPTIONS=""

# Fonction d'aide
show_help() {
    echo -e "${BLUE}Usage: ${0} [OPTIONS]"
    echo -e "ExÃ©cute le pipeline d'analyse HEPMASS\n"
    echo -e "Options:"
    echo -e "  --enable-logs       Active les logs Spark dÃ©taillÃ©s"
    echo -e "  --no-services       Ne pas gÃ©rer les services HDFS/Spark"
    echo -e "  --disable-safe-mode DÃ©sactive le safe mode HDFS"
    echo -e "  --help              Affiche ce message d'aide"
    echo -e "  --version           Affiche la version du script${NC}"
    exit 0
}

# Gestion des erreurs
handle_error() {
    echo -e "${RED}âœ– Erreur critique Ã  l'Ã©tape $1${NC}"
    exit 1
}

# Configuration des logs
configure_logging() {
    if [ "${ENABLE_LOGS}" = false ]; then
        cat > "${LOG_CONFIG}" << EOF
log4j.rootCategory=ERROR, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

log4j.logger.org.apache.spark=ERROR
log4j.logger.org.apache.hadoop=ERROR
log4j.logger.io.netty=ERROR
EOF
        SPARK_LOG_OPTIONS="--conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:${LOG_CONFIG} \
                          --conf spark.executor.extraJavaOptions=-Dlog4j.configuration=file:${LOG_CONFIG}"
    fi
}

# Nettoyage
cleanup() {
    [ -f "${LOG_CONFIG}" ] && rm -f "${LOG_CONFIG}"
    if [ "${MANAGE_SERVICES}" = true ]; then
        confirm_shutdown
    fi
}

# Fonction de confirmation manuelle
confirm_shutdown() {
    echo -e "\n${YELLOW}âš  Les services HDFS/Spark sont toujours en cours d'exÃ©cution${NC}"
    while true; do
        read -p "Voulez-vous arrÃªter les services maintenant ? [y/N] " response
        case "$response" in
            [yY][eE][sS]|[yY])
                stop_services
                break
                ;;
            [nN][oO]|[nN]|"")
                echo -e "${YELLOW}âš  Les services seront laissÃ©s en cours d'exÃ©cution${NC}"
                break
                ;;
            *)
                echo -e "${RED}âŒ RÃ©ponse invalide. Veuillez rÃ©pondre par 'y' (oui) ou 'n' (non).${NC}"
                ;;
        esac
    done
}

# Fonctions HDFS
start_hdfs() {
    echo -e "${BLUE}â†’ DÃ©marrage HDFS...${NC}"
    "${HADOOP_SBIN}/start-dfs.sh" || handle_error "DÃ©marrage HDFS"
    echo -e "${GREEN}âœ” HDFS dÃ©marrÃ©${NC}"
}

stop_hdfs() {
    echo -e "${BLUE}â†’ ArrÃªt HDFS...${NC}"
    "${HADOOP_SBIN}/stop-dfs.sh" || echo -e "${RED}âš  Ã‰chec de l'arrÃªt HDFS${NC}"
    echo -e "${GREEN}âœ” HDFS arrÃªtÃ©${NC}"
}

disable_hdfs_safemode() {
    echo -e "\n${YELLOW}ðŸ›¡  DÃ©sactivation du safe mode HDFS...${NC}"
    hdfs dfsadmin -safemode leave || handle_error "DÃ©sactivation safe mode"
    echo -e "${GREEN}âœ” Safe mode dÃ©sactivÃ©${NC}"
}

# Fonctions Spark
start_spark() {
    echo -e "${BLUE}â†’ DÃ©marrage Spark Master...${NC}"
    "${SPARK_SBIN}/start-master.sh" || handle_error "DÃ©marrage Spark Master"
    echo -e "${GREEN}âœ” Spark Master dÃ©marrÃ©${NC}"

    echo -e "${BLUE}â†’ DÃ©marrage Spark Workers...${NC}"
    "${SPARK_SBIN}/start-workers.sh" || handle_error "DÃ©marrage Spark Workers"
    echo -e "${GREEN}âœ” Spark Workers dÃ©marrÃ©s${NC}"
}

stop_spark() {
    echo -e "${BLUE}â†’ ArrÃªt Spark Workers...${NC}"
    "${SPARK_SBIN}/stop-workers.sh" || echo -e "${RED}âš  Ã‰chec de l'arrÃªt Workers Spark${NC}"
    echo -e "${GREEN}âœ” Spark Workers arrÃªtÃ©s${NC}"

    echo -e "${BLUE}â†’ ArrÃªt Spark Master...${NC}"
    "${SPARK_SBIN}/stop-master.sh" || echo -e "${RED}âš  Ã‰chec de l'arrÃªt Master Spark${NC}"
    echo -e "${GREEN}âœ” Spark Master arrÃªtÃ©${NC}"
}

# Gestion des services
start_services() {
    echo -e "${YELLOW}âš™ DÃ©marrage des services...${NC}"
    start_hdfs
    start_spark
    
    if [ "${DISABLE_SAFEMODE}" = true ]; then
        disable_hdfs_safemode
    fi
}

stop_services() {
    echo -e "\n${YELLOW}ðŸ›‘ ArrÃªt des services...${NC}"
    stop_spark
    stop_hdfs
}

# VÃ©rification HDFS
verify_hdfs() {
    echo -e "\n${YELLOW}ðŸ” VÃ©rification des fichiers HDFS...${NC}"
    
    local files=("all_train.csv.gz" "all_test.csv.gz")
    for file in "${files[@]}"; do
        hdfs dfs -test -e "${HDFS_BASE_PATH}/${file}" || {
            echo -e "${RED}âœ– Fichier manquant: ${file}${NC}"
            exit 1
        }
    done
    
    echo -e "${GREEN}âœ” Tous les fichiers requis sont prÃ©sents${NC}"
}

# Nettoyage HDFS
clean_hdfs() {
    echo -e "\n${YELLOW}ðŸ§¹ Nettoyage des prÃ©cÃ©dents rÃ©sultats...${NC}"
    
    local dirs=("parsed_train" "parsed_test" "scaled_train" "scaled_test" "models")
    for dir in "${dirs[@]}"; do
        if hdfs dfs -test -e "${HDFS_BASE_PATH}/${dir}"; then
            echo -e "Suppression de ${BLUE}${dir}${NC}"
            hdfs dfs -rm -r "${HDFS_BASE_PATH}/${dir}" || handle_error "Nettoyage HDFS"
        fi
    done
}

# ExÃ©cution d'une Ã©tape
run_step() {
    local step_num=$1
    local step_name=$2
    local script=$3
    
    echo -e "\n${GREEN}=== Ã‰tape ${step_num}/7: ${step_name} ===${NC}"
    echo -e "${YELLOW}ExÃ©cution de ${BLUE}${script}${YELLOW}...${NC}"
    
    spark-submit ${SPARK_LOG_OPTIONS} "${SCRIPT_DIR}/${script}" || handle_error "Ã‰tape ${step_num}"
}

# Traitement des arguments
while [[ $# -gt 0 ]]; do
    case "$1" in
        --enable-logs)
            ENABLE_LOGS=true
            shift
            ;;
        --no-services)
            MANAGE_SERVICES=false
            shift
            ;;
        --disable-safe-mode)
            DISABLE_SAFEMODE=true
            shift
            ;;
        --help)
            show_help
            ;;
        --version)
            echo -e "${BLUE}HEPMASS Pipeline v${VERSION}${NC}"
            exit 0
            ;;
        *)
            echo -e "${RED}Option inconnue: $1${NC}"
            show_help
            exit 1
            ;;
    esac
done

# Initialisation
trap cleanup EXIT SIGINT SIGTERM
clear
echo -e "${BLUE}=============================================="
echo -e " Pipeline d'analyse HEPMASS - v${VERSION}"
echo -e "==============================================${NC}"

# Configuration initiale
configure_logging

# Gestion des services
if [ "${MANAGE_SERVICES}" = true ]; then
    start_services
else
    echo -e "${YELLOW}âš  Gestion des services dÃ©sactivÃ©e${NC}"
fi

# VÃ©rifications HDFS
verify_hdfs
clean_hdfs

# ExÃ©cution du pipeline
echo -e "\n${YELLOW}ðŸš€ DÃ©marrage du pipeline...${NC}"

run_step 1 "Chargement des donnÃ©es" "1_data_loading.py"
run_step 2 "Exploration des donnÃ©es" "2_data_exploration.py"
run_step 3 "PrÃ©traitement" "3_preprocessing.py"
run_step 4 "EntraÃ®nement des modÃ¨les" "4_model_training.py"
run_step 5 "Ã‰valuation des modÃ¨les" "5_evaluation.py"
run_step 6 "Optimisation hyperparamÃ¨tres" "6_hyperparameter_tuning.py"
run_step 7 "Visualisation finale" "7_visualization.py"

# Finalisation
echo -e "\n${BLUE}=============================================="
echo -e "âœ… Pipeline exÃ©cutÃ© avec succÃ¨s!"
if [ "${MANAGE_SERVICES}" = true ]; then
    echo -e "âš  Les services HDFS/Spark sont toujours en cours d'exÃ©cution${NC}"
    confirm_shutdown
else
    echo -e "==============================================${NC}"
fi